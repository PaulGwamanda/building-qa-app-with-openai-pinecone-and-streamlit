{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **How to ingest data to Pinecone using LangChain**\n",
    "\n",
    "- This notebook shows you how you can ingest data from a document into a Pinecone Database.\n",
    "- It uses Langchain to ease the process of split and ingest the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Import dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pinecone\n",
    "import openai\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Get environment variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_API_ENV = os.getenv('PINECONE_ENVIRONMENT')\n",
    "PINECONE_INDEX_NAME = 'qa-app'\n",
    "OPENAI_API_KEY = os.getenv('AZURE_OPENAI_APIKEY')\n",
    "OPENAI_API_BASE = os.getenv('AZURE_OPENAI_BASE_URI')\n",
    "OPENAI_API_TYPE = 'azure'\n",
    "OPENAI_API_VERSION = '2023-03-15-preview'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Load your data files and split it into chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "detectron2 is not installed. Cannot use the hi_res partitioning strategy. Falling back to partitioning with the fast strategy.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now have a total of 917 chunks\n"
     ]
    }
   ],
   "source": [
    "loader = UnstructuredPDFLoader(\"./docs/NET-Microservices-Architecture-for-Containerized-NET-Applications.pdf\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "\n",
    "print (f'You have a total of {len(chunks)} chunks')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Init Azure OpenAI and Pinecone clients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_base=OPENAI_API_BASE, openai_api_key=OPENAI_API_KEY, openai_api_type=OPENAI_API_TYPE, chunk_size=1)\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,\n",
    "    environment=PINECONE_API_ENV  \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Create and store embeddings into Pinecone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Pinecone.from_texts([t.page_content for t in chunks], embeddings, index_name=PINECONE_INDEX_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Ask a question**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \n",
      " 1. Use asynchronous communication: Implement message-based communication across internal microservices to minimize ripple effects and enforce a higher level of microservice autonomy.\n",
      "\n",
      "2. Implement retries with exponential backoff: In case of transient faults, use the \"Retry pattern\" to retry the operation with increasing time intervals between attempts, allowing the system to recover from temporary issues.\n",
      "\n",
      "3. Use the Circuit Breaker pattern: Track the number of failed requests, and if the error rate exceeds a configured limit, trip a \"circuit breaker\" to prevent further attempts. After a timeout period, try again and close the circuit breaker if new requests are successful.\n"
     ]
    }
   ],
   "source": [
    "llm = AzureChatOpenAI(temperature=0, openai_api_base=OPENAI_API_BASE, openai_api_key=OPENAI_API_KEY, openai_api_version=OPENAI_API_VERSION, deployment_name='gpt-4')\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "query = \"Enumerate 3 strategies to handle partial failures?\"\n",
    "docs = docsearch.similarity_search(query, include_metadata=True)\n",
    "\n",
    "result = chain.run(input_documents=docs, question=query)\n",
    "\n",
    "print(f\"Answer: \\n {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
